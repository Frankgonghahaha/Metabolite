import pandas as pd
import os
import networkx as nx
from itertools import permutations, islice
import json
import re
import csv

# --- 0. å®‰è£…ä¾èµ–åº“ ---
# pip install python-libsbml networkx pandas

try:
    from libsbml import readSBML
except ImportError:
    print("=" * 80);
    print("âŒ è‡´å‘½é”™è¯¯: å¿…éœ€çš„åº“ 'python-libsbml' æœªæ‰¾åˆ°ã€‚");
    print("è¯·å…ˆåœ¨æ‚¨çš„ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…å®ƒ:");
    print("pip install python-libsbml networkx pandas");
    print("=" * 80);
    exit()

# --- 1. å‚æ•°è®¾ç½® (å·²æ ¹æ®æ‚¨çš„è¦æ±‚æ›´æ–°) ---
MODELS_DIR = '/Volumes/CC/å…¨çƒ+å›½å†…96s/INAP_models'
BIGG_METABOLITES_PATH = '/Volumes/CC/å…¨çƒ+å›½å†…96s/INAP è®¡ç®—è¡¨/bigg_models_metabolites.txt'
CACHE_FILE_PATH = '/Volumes/CC/å…¨çƒ+å›½å†…96s/INAP è®¡ç®—è¡¨/seed_set_cache.json'
OUTPUT_PTM_FILE = '/Volumes/CC/å…¨çƒ+å›½å†…96s/INAP è®¡ç®—è¡¨/Step4_PTM_results.csv'


# --- 2. æ ¸å¿ƒç®—æ³• (æ— å˜åŠ¨) ---
def buildDG(sbml_file_path):
    document = readSBML(sbml_file_path)
    if document.getNumErrors() > 0: pass
    model = document.getModel()
    if model is None: return None
    DG = nx.DiGraph()
    for r in model.getListOfReactions():
        reactants = {i.getSpecies() for i in r.getListOfReactants()}
        products = {j.getSpecies() for j in r.getListOfProducts()}
        for reactant in reactants:
            for product in products: DG.add_edge(reactant, product)
    return DG


def getSeedSet(DG, maxComponentSize=5):
    if DG is None: return set(), set()
    SeedSetConfidence = {}
    for component in nx.weakly_connected_components(DG):
        subgraph = DG.subgraph(component)
        for scc in nx.strongly_connected_components(subgraph):
            scc_list = list(scc)
            if len(scc_list) > maxComponentSize: continue
            is_seed_component = True
            for node in scc_list:
                for pred in DG.predecessors(node):
                    if pred not in scc_list: is_seed_component = False; break
                if not is_seed_component: break
            if is_seed_component:
                confidence = 1.0 / len(scc_list) if scc_list else 1.0
                for node in scc_list: SeedSetConfidence[node] = confidence
    SeedSet = set(SeedSetConfidence.keys())
    nonSeedSet = set(DG.nodes()) - SeedSet
    return SeedSet, nonSeedSet


def get_clean_bigg_id(raw_id):
    cleaned_id = raw_id[2:] if raw_id.startswith('M_') else raw_id
    cleaned_id = re.sub(r'_[a-z0-9]$', '', cleaned_id)
    return cleaned_id


# --- 3. ä¸»æ‰§è¡Œæµç¨‹ (å†…å­˜å®‰å…¨ç‰ˆ) ---
def main():
    # --- æ­¥éª¤A: åŠ è½½æˆ–è®¡ç®—ç§å­é›† ---
    if not os.path.isdir(MODELS_DIR): print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹ç›®å½• '{MODELS_DIR}'"); return

    # ç¡®ä¿ç¼“å­˜æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•å­˜åœ¨
    cache_dir = os.path.dirname(CACHE_FILE_PATH)
    os.makedirs(cache_dir, exist_ok=True)

    model_data = {}
    if os.path.exists(CACHE_FILE_PATH):
        print(f"âœ… å‘ç°ç¼“å­˜æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½ç§å­é›†...");
        with open(CACHE_FILE_PATH, 'r') as f:
            cached_data = json.load(f)
            for model_id, data in cached_data.items():
                model_data[model_id] = {'seeds': set(data['seeds']), 'non_seeds': set(data['non_seeds'])}
        print("ç§å­é›†åŠ è½½å®Œæ¯•ã€‚")
    else:
        print("æœªæ‰¾åˆ°ç¼“å­˜ï¼Œå°†æ‰§è¡Œå®Œæ•´è®¡ç®—...")
        print(f"\næ­¥éª¤1: æ­£åœ¨ä¸º '{MODELS_DIR}' ä¸­çš„æ‰€æœ‰æ¨¡å‹è®¡ç®—ç§å­é›†...")
        model_files = [f for f in os.listdir(MODELS_DIR) if f.lower().endswith('.xml')]
        for i, filename in enumerate(model_files):
            model_id = os.path.splitext(filename)[0]
            print(f"  å¤„ç†ä¸­ ({i + 1}/{len(model_files)}): {model_id}")
            file_path = os.path.join(MODELS_DIR, filename)
            dg = buildDG(file_path)
            seed_set, non_seed_set = getSeedSet(dg)
            model_data[model_id] = {'seeds': seed_set, 'non_seeds': non_seed_set}
        print("æ‰€æœ‰æ¨¡å‹çš„ç§å­é›†è®¡ç®—å®Œæˆã€‚")
        print(f"æ­£åœ¨å°†è®¡ç®—ç»“æœä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶ '{os.path.basename(CACHE_FILE_PATH)}'...")
        serializable_data = {mid: {'seeds': list(d['seeds']), 'non_seeds': list(d['non_seeds'])} for mid, d in
                             model_data.items()}
        with open(CACHE_FILE_PATH, 'w') as f:
            json.dump(serializable_data, f, indent=4)
        print("ç¼“å­˜æ–‡ä»¶å·²åˆ›å»ºã€‚")

    # --- æ­¥éª¤B: åŠ è½½ä»£è°¢ç‰©åç§° ---
    if not os.path.exists(BIGG_METABOLITES_PATH): print(
        f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°BiGGä»£è°¢ç‰©æ³¨é‡Šæ–‡ä»¶ '{BIGG_METABOLITES_PATH}'"); return
    print(f"\næ­£åœ¨åŠ è½½ä»£è°¢ç‰©åç§°...");
    bigg_df = pd.read_csv(BIGG_METABOLITES_PATH, sep='\t', header=0, usecols=['universal_bigg_id', 'name'], dtype=str)
    bigg_df.dropna(subset=['universal_bigg_id'], inplace=True)
    metabolite_names = pd.Series(bigg_df.name.values, index=bigg_df.universal_bigg_id).to_dict()
    print(f"  âœ… æˆåŠŸåŠ è½½ {len(metabolite_names)} ä¸ªä»£è°¢ç‰©åç§°ã€‚")

    # --- æ­¥éª¤C: æµå¼å¤„ç†å’Œå†™å…¥ ---
    print("\næ­¥éª¤2: æ­£åœ¨æ¯”è¾ƒæ¨¡å‹é…å¯¹å¹¶ç›´æ¥å†™å…¥æ–‡ä»¶ (å†…å­˜å®‰å…¨æ¨¡å¼)...")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(OUTPUT_PTM_FILE)
    os.makedirs(output_dir, exist_ok=True)

    total_pairs = len(model_data) * (len(model_data) - 1)
    processed_count = 0
    ptm_found_count = 0

    with open(OUTPUT_PTM_FILE, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['Donor', 'Receptor', 'PTM_ID', 'PTM_Name'])

        for donor_id, receptor_id in permutations(model_data.keys(), 2):
            ptms = model_data[donor_id]['seeds'].intersection(model_data[receptor_id]['non_seeds'])

            if ptms:
                for ptm in ptms:
                    cleaned_id = get_clean_bigg_id(ptm)
                    name = metabolite_names.get(cleaned_id)
                    writer.writerow([donor_id, receptor_id, ptm, name if name else 'N/A'])
                    ptm_found_count += 1

            processed_count += 1
            if processed_count % 100000 == 0:
                print(f"  å·²å¤„ç† {processed_count} / {total_pairs} ä¸ªé…å¯¹...")

    print(f"\næ­¥éª¤3: æ‰€æœ‰é…å¯¹å¤„ç†å®Œæ¯•ã€‚")
    print("\n" + "=" * 50)
    print("ğŸ‰ å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")
    print(f"å…±æ‰¾åˆ°å¹¶å†™å…¥ {ptm_found_count} æ¡æ½œåœ¨è½¬ç§»å…³ç³»ã€‚")
    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åœ¨:\n{OUTPUT_PTM_FILE}")
    print("=" * 50)


if __name__ == "__main__":
    main()